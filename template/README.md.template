# {{PROJECT_NAME}}

A desktop AI/ML application built with [Tether](https://github.com/your-org/tether).

## Requirements

| Tool | Version | Installation |
|------|---------|--------------|
| Node.js | 18+ | [nodejs.org](https://nodejs.org/) |
| pnpm | 8+ | `npm install -g pnpm` |
| Python | 3.11+ | [python.org](https://www.python.org/) |
| uv | latest | [docs.astral.sh/uv](https://docs.astral.sh/uv/) |
| Rust | latest | [rustup.rs](https://rustup.rs/) |

For Ollama backend (default):
- [Ollama](https://ollama.com/) installed and running

## Quick Start

```bash
# Install dependencies
pnpm install

# Start development (frontend + backend)
pnpm dev:all
```

Open http://localhost:5173 in your browser.

## Commands

### Development

| Command | Description |
|---------|-------------|
| `pnpm dev` | Start frontend only (Vite) |
| `pnpm dev:py` | Start Python backend only |
| `pnpm dev:all` | Start both frontend and backend |
| `pnpm tauri:dev` | Start as desktop app (with hot reload) |

### Building

| Command | Description |
|---------|-------------|
| `pnpm build` | Build frontend |
| `pnpm python:build` | Build Python backend (PyInstaller) |
| `pnpm build:app` | Build complete desktop app |

The built app will be in `src-tauri/target/release/bundle/`.

## Configuration

Copy `.env.example` to `.env`:

```bash
cp .env.example .env
```

### Ollama (Default)

1. Install [Ollama](https://ollama.com/)
2. Pull a model:
   ```bash
   ollama pull llama3.2
   ```
3. Start Ollama:
   ```bash
   ollama serve
   ```

### Local LLM (Embedded)

1. Download a GGUF model from [Hugging Face](https://huggingface.co/models?library=gguf)
2. Set in `.env`:
   ```
   TETHER_LLM_BACKEND=local
   TETHER_MODEL_PATH=./models/your-model.gguf
   ```

### OpenAI API (Experimental)

> **Note:** OpenAI support is experimental and not thoroughly tested.

1. Get an API key from [OpenAI](https://platform.openai.com/)
2. Set in `.env`:
   ```
   TETHER_LLM_BACKEND=openai
   OPENAI_API_KEY=sk-...
   ```

## Supported Models

| Backend | Models | Notes |
|---------|--------|-------|
| **Ollama** | llama3.2, gemma3, qwen3, deepseek-r1, mistral, etc. | See [ollama.com/library](https://ollama.com/library) |
| **Local LLM** | Any GGUF model | See [Hugging Face GGUF models](https://huggingface.co/models?library=gguf) |
| **OpenAI** | gpt-4o, gpt-4o-mini, gpt-4-turbo | Experimental |

### Thinking Models

Models with reasoning capabilities (like `deepseek-r1`, `qwen3`) support "thinking mode" which shows the model's reasoning process. This is automatically enabled when available.

## Project Structure

```
{{PROJECT_NAME}}/
├── frontend/             # React frontend
│   ├── components/       # React components
│   ├── hooks/            # Custom hooks
│   └── App.tsx           # Main app component
├── backend/              # Python backend
│   ├── app/              # FastAPI application
│   │   ├── routes/       # API endpoints
│   │   └── services/     # Business logic
│   └── scripts/          # Build scripts
└── src-tauri/            # Tauri (Rust) shell
    └── src/              # Rust source code
```

## License

MIT License
